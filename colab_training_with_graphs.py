# Google Colab'da Trafik Tahmin Modeli EÄŸitimi ve GÃ¶rselleÅŸtirme
# Bu kodu Google Colab'da Ã§alÄ±ÅŸtÄ±rÄ±n

# Gerekli kÃ¼tÃ¼phaneleri iÃ§e aktar
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score
from sklearn.metrics import precision_recall_curve, roc_curve, auc
import joblib
import warnings
warnings.filterwarnings('ignore')

# Grafik ayarlarÄ±
plt.style.use('seaborn-v0_8')
sns.set_palette("husl")
plt.rcParams['figure.figsize'] = (12, 8)
plt.rcParams['font.size'] = 12

print("ğŸš€ Trafik Tahmin Modeli EÄŸitimi ve Analizi BaÅŸlÄ±yor...")
print("=" * 60)

# Veri dosyalarÄ±nÄ± yÃ¼kle
dosyalar = [
    "ibb_traffic_2024_11.csv",
    "ibb_traffic_2024_12.csv",
    "ibb_traffic_2025_01.csv"
]

print(f"ğŸ“‹ YÃ¼klenecek dosyalar: {dosyalar}")
print("-" * 40)

# Verileri birleÅŸtir
df_list = []
for dosya in dosyalar:
    try:
        df = pd.read_csv(dosya)
        df_list.append(df)
        print(f"âœ… {dosya} baÅŸarÄ±yla yÃ¼klendi - {df.shape[0]} satÄ±r, {df.shape[1]} sÃ¼tun")
    except Exception as e:
        print(f"âŒ {dosya} yÃ¼klenirken hata: {str(e)}")

if df_list:
    df = pd.concat(df_list, ignore_index=True)
    print(f"\nğŸ“Š BirleÅŸtirilmiÅŸ veri seti boyutu: {df.shape[0]} satÄ±r, {df.shape[1]} sÃ¼tun")
else:
    print("âŒ HiÃ§ dosya yÃ¼klenemedi!")
    exit()

# Veri sÃ¶zlÃ¼ÄŸÃ¼ne gÃ¶re sÃ¼tun kontrolÃ¼
expected_columns = ['DATE_TIME', 'LATITUDE', 'LONGITUDE', 'GEOHASH', 
                   'MINIMUM_SPEED', 'MAXIMUM_SPEED', 'AVERAGE_SPEED', 'NUMBER_OF_VEHICLES']

print("\nğŸ” SÃ¼tun kontrolÃ¼:")
missing_columns = [col for col in expected_columns if col not in df.columns]
if missing_columns:
    print(f"âŒ Eksik sÃ¼tunlar: {missing_columns}")
    print(f"ğŸ“‹ Mevcut sÃ¼tunlar: {list(df.columns)}")
else:
    print("âœ… TÃ¼m gerekli sÃ¼tunlar mevcut")

# Veri tiplerini kontrol et ve dÃ¼zenle
print("\nğŸ”§ Veri tipleri dÃ¼zenleniyor...")
df['DATE_TIME'] = pd.to_datetime(df['DATE_TIME'])
df['LATITUDE'] = pd.to_numeric(df['LATITUDE'], errors='coerce')
df['LONGITUDE'] = pd.to_numeric(df['LONGITUDE'], errors='coerce')
df['MINIMUM_SPEED'] = pd.to_numeric(df['MINIMUM_SPEED'], errors='coerce')
df['MAXIMUM_SPEED'] = pd.to_numeric(df['MAXIMUM_SPEED'], errors='coerce')
df['AVERAGE_SPEED'] = pd.to_numeric(df['AVERAGE_SPEED'], errors='coerce')
df['NUMBER_OF_VEHICLES'] = pd.to_numeric(df['NUMBER_OF_VEHICLES'], errors='coerce')

# Eksik deÄŸerleri temizle
df = df.dropna()
print(f"âœ… Temizlenen veri: {df.shape[0]} satÄ±r")

# Veri Ã¶zeti
print("\nğŸ“ˆ VERÄ° Ã–ZETÄ°:")
print("-" * 30)
print(f"ğŸ“… Tarih aralÄ±ÄŸÄ±: {df['DATE_TIME'].min()} - {df['DATE_TIME'].max()}")
print(f"ğŸ—ºï¸  Koordinat aralÄ±ÄŸÄ±: Lat({df['LATITUDE'].min():.4f} - {df['LATITUDE'].max():.4f})")
print(f"    Lon({df['LONGITUDE'].min():.4f} - {df['LONGITUDE'].max():.4f})")
print(f"ğŸš— HÄ±z aralÄ±ÄŸÄ±: {df['AVERAGE_SPEED'].min():.1f} - {df['AVERAGE_SPEED'].max():.1f} km/h")
print(f"ğŸš™ AraÃ§ sayÄ±sÄ±: {df['NUMBER_OF_VEHICLES'].min()} - {df['NUMBER_OF_VEHICLES'].max()}")
print(f"ğŸ“ Benzersiz GEOHASH: {df['GEOHASH'].nunique()}")

# Veri Ã¶n iÅŸleme
print("\nğŸ” Veri Ã¶n iÅŸleme baÅŸlÄ±yor...")

# Zaman Ã¶zelliklerini oluÅŸtur
df["timestamp"] = pd.to_datetime(df["DATE_TIME"])
df["hour"] = df["timestamp"].dt.hour
df["day_of_week"] = df["timestamp"].dt.dayofweek
df["is_weekend"] = (df["day_of_week"] >= 5).astype(int)
df["month"] = df["timestamp"].dt.month

# Trafik seviyesi sÄ±nÄ±flandÄ±rmasÄ± - Daha dengeli eÅŸikler
def classify_traffic(avg_speed):
    if avg_speed <= 30:      # Daha yÃ¼ksek eÅŸik
        return 2  # YoÄŸun
    elif avg_speed <= 50:    # Daha yÃ¼ksek eÅŸik
        return 1  # Orta
    else:
        return 0  # Az

df["traffic_level"] = df["AVERAGE_SPEED"].apply(classify_traffic)

# SÄ±nÄ±f daÄŸÄ±lÄ±mÄ±nÄ± kontrol et
print("\nğŸ“Š SÄ±nÄ±f daÄŸÄ±lÄ±mÄ±:")
traffic_counts = df["traffic_level"].value_counts().sort_index()
for level, count in traffic_counts.items():
    level_name = ['Az', 'Orta', 'YoÄŸun'][level]
    print(f"  {level} ({level_name}): {count:,} ({count/len(df)*100:.1f}%)")

# Veri dengesizliÄŸini dÃ¼zelt
print("\nâš–ï¸ Veri dengesizliÄŸi dÃ¼zeltiliyor...")
df_0 = df[df["traffic_level"] == 0]
df_1 = df[df["traffic_level"] == 1]
df_2 = df[df["traffic_level"] == 2]

n = min(len(df_0), len(df_1), len(df_2))
print(f"Her sÄ±nÄ±f iÃ§in {n:,} Ã¶rnek kullanÄ±lacak")

df_balanced = pd.concat([
    df_0.sample(n=n, random_state=42),
    df_1.sample(n=n, random_state=42),
    df_2.sample(n=n, random_state=42)
]).sample(frac=1, random_state=42)

print(f"âœ… Dengeli veri seti: {df_balanced.shape[0]:,} satÄ±r")

# Ã–zellik seÃ§imi
features = [
    "hour", "day_of_week", "is_weekend", "month",
    "MINIMUM_SPEED", "MAXIMUM_SPEED", "NUMBER_OF_VEHICLES",
    "LATITUDE", "LONGITUDE"
]

X = df_balanced[features]
y = df_balanced["traffic_level"]

# Veriyi bÃ¶l
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
print(f"\nğŸ“Š EÄŸitim seti: {X_train.shape[0]:,} Ã¶rnek")
print(f"ğŸ“Š Test seti: {X_test.shape[0]:,} Ã¶rnek")

# Ã–zellik Ã¶lÃ§eklendirme
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Model eÄŸitimi - Daha iyi parametreler
print("\nğŸ¤– Model eÄŸitimi baÅŸlÄ±yor...")
model = RandomForestClassifier(
    n_estimators=200,
    max_depth=15,
    min_samples_split=5,
    min_samples_leaf=2,
    random_state=42,
    n_jobs=-1  # Paralel iÅŸlem
)
model.fit(X_train_scaled, y_train)
print("âœ… Model eÄŸitimi tamamlandÄ±!")

# Model deÄŸerlendirmesi
print("\nğŸ“Š Model performansÄ±:")
y_pred = model.predict(X_test_scaled)
accuracy = accuracy_score(y_test, y_pred)
print(f"ğŸ¯ DoÄŸruluk: {accuracy:.3f} ({accuracy*100:.1f}%)")
print("\nğŸ“‹ DetaylÄ± Rapor:")
print(classification_report(y_test, y_pred, target_names=['Az', 'Orta', 'YoÄŸun'])) 